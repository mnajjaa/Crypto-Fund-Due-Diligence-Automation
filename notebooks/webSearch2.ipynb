{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c7800b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from newspaper import Article\n",
    "\n",
    "# -------- CONFIG --------\n",
    "SERPER_API_KEY = \"b6377b6d2d07fe6a6f334f03986986351846d4bb\"  # üîÅ Replace with your actual key\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d694248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c6e1158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.51.0)\n",
      "Requirement already satisfied: pyee<13,>=12 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from playwright) (12.1.1)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from playwright) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyee<13,>=12->playwright) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install playwright\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "479046eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (2024.12.14)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (25.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2dd032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_text(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return f\"Source: {url}\\n\\n{article.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Source: {url}\\n\\n[Failed to extract: {e}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "636ee79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llama3_locally(prompt):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Accept\": \"application/json\"\n",
    "        }\n",
    "        response = requests.post(\n",
    "            OLLAMA_API_URL,\n",
    "            json={\"model\": \"llama3\", \"prompt\": prompt, \"stream\": False},\n",
    "            headers=headers,\n",
    "            verify=False  # Disable SSL verification for ngrok URL\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "                answer_text = response.json().get(\"response\", \"[No response found]\")\n",
    "                return answer_text  # ‚úÖ Return only the clean text\n",
    "        else:\n",
    "            return f\"[HTTP {response.status_code}] Error from Llama3\"\n",
    "    except Exception as e:\n",
    "        return f\"[‚ùå Error calling Llama3: {e}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74bb5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f88fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66cd05af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    texts = soup.find_all(text=True)\n",
    "\n",
    "    blacklist = [\n",
    "        '[document]', 'noscript', 'header', 'html', 'meta', 'head', 'input', 'script', 'style'\n",
    "    ]\n",
    "\n",
    "    output = ''\n",
    "    for t in texts:\n",
    "        if t.parent.name not in blacklist:\n",
    "            content = t.strip()\n",
    "            if content:\n",
    "                output += f'{content} '\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32a0714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_url_trusted_by_ai(url):\n",
    "    prompt = f\"\"\"You are a trustworthiness evaluator.\n",
    "\n",
    "Decide whether the following website is generally considered a **professional and trustworthy source of information** for serious research, financial news, or due diligence.\n",
    "\n",
    "Do **not** include websites that:\n",
    "- Are editable by the public (like Wikipedia, Reddit, Quora)\n",
    "- Are known for unverified or user-generated content\n",
    "\n",
    "Only reply with `yes` or `no`.\n",
    "\n",
    "### Website:\n",
    "{url}\n",
    "\n",
    "### Is this source trusted?\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Accept\": \"application/json\"\n",
    "        }\n",
    "        response = requests.post(\n",
    "            OLLAMA_API_URL,\n",
    "            json={\"model\": \"llama3\", \"prompt\": prompt, \"stream\": False},\n",
    "            headers=headers,\n",
    "            verify=False\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            verdict = response.json().get(\"response\", \"\").strip().lower()\n",
    "            print(f\"ü§ñ AI verdict for {url}: {verdict}\")\n",
    "            return \"yes\" in verdict\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"[AI trust check failed for {url}]: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c40b2a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search_serper(query, max_results=4):\n",
    "    headers = {\n",
    "        \"X-API-KEY\": SERPER_API_KEY\n",
    "    }\n",
    "    json_data = {\n",
    "        \"q\": query\n",
    "    }\n",
    "    response = requests.post(\"https://google.serper.dev/search\", headers=headers, json=json_data)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå Serper API error: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    data = response.json()\n",
    "    results = data.get(\"organic\", [])\n",
    "    urls = []\n",
    "    for item in results:\n",
    "        url = item[\"link\"]\n",
    "        print(f\"üîç Evaluating source: {url}\")\n",
    "        if is_url_trusted_by_ai(url):\n",
    "            urls.append(url)\n",
    "        if len(urls) >= max_results:\n",
    "            break\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ba2d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_question_type(query):\n",
    "    classification_prompt = f\"\"\"You are a classification assistant.\n",
    "\n",
    "Decide if the following question requires a short, direct answer (like a number, name, or fact), or a detailed, multi-paragraph summary.\n",
    "\n",
    "Respond with only one word: `short` or `detailed`.\n",
    "\n",
    "### Question:\n",
    "{query}\n",
    "\n",
    "### Answer Style:\"\"\"\n",
    "\n",
    "    # Send to LLaMA3\n",
    "    try:\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Accept\": \"application/json\"\n",
    "        }\n",
    "        response = requests.post(\n",
    "            OLLAMA_API_URL,\n",
    "            json={\"model\": \"llama3\", \"prompt\": classification_prompt, \"stream\": False},\n",
    "            headers=headers,\n",
    "            verify=False\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            style = response.json().get(\"response\", \"\").strip().lower()\n",
    "            return \"short\" if \"short\" in style else \"detailed\"\n",
    "        else:\n",
    "            return \"detailed\"\n",
    "    except:\n",
    "        return \"detailed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cf340ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_final_answer(answer, query, urls):\n",
    "    print(\"tetete\"+answer)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"üß† Answer to: {query}\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüîó Sources Used:\")\n",
    "    for url in urls:\n",
    "        print(f\" - {url}\")\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b5c1553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "def extract_readable_text(url):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")  # New headless mode is less detectable\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    # Fake being a real user\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    # Optional: make webdriver undetectable\n",
    "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    chrome_options.add_experimental_option('useAutomationExtension', False)\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    driver.execute_cdp_cmd(\n",
    "        \"Page.addScriptToEvaluateOnNewDocument\",\n",
    "        {\n",
    "            \"source\": \"\"\"\n",
    "                Object.defineProperty(navigator, 'webdriver', {\n",
    "                  get: () => undefined\n",
    "                })\n",
    "            \"\"\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # let JS render\n",
    "\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    texts = soup.find_all(string=True)  # Fixed here\n",
    "\n",
    "    output = ''\n",
    "    blacklist = ['[document]', 'noscript', 'header', 'html', 'meta', 'head', 'input', 'script', 'style']\n",
    "\n",
    "    for t in texts:\n",
    "        if t.parent.name not in blacklist:\n",
    "            output += '{} '.format(t.strip())\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14247235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def smart_clean_text(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    important_lines = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        # Filter rules:\n",
    "        if \"cookie\" in line.lower():\n",
    "            continue\n",
    "        if \"privacy\" in line.lower():\n",
    "            continue\n",
    "        if \"consent\" in line.lower():\n",
    "            continue\n",
    "        if \"sign up\" in line.lower() or \"sign in\" in line.lower():\n",
    "            continue\n",
    "        if \"accept\" in line.lower() and \"cookies\" in line.lower():\n",
    "            continue\n",
    "        if len(line) < 5:\n",
    "            continue\n",
    "        important_lines.append(line)\n",
    "\n",
    "    cleaned_text = \"\\n\".join(important_lines)\n",
    "    return cleaned_text\n",
    "\n",
    "# Usage\n",
    "\n",
    "# Then build prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a229b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_rag_query(query):\n",
    "    print(f\"üîç Searching with Serper for: {query}\")\n",
    "    urls = web_search_serper(query)\n",
    "\n",
    "    # --- Blacklist bad URLs ---\n",
    "    blacklist = [\n",
    "        \"https://bitflyer.com/en-us/bitcoin-chart\"\n",
    "    ]\n",
    "\n",
    "    # Remove blacklisted URLs\n",
    "    urls = [url for url in urls if url not in blacklist]\n",
    "\n",
    "    print(f\"üìé URLs after blacklist filter: {urls}\")\n",
    "    if not urls:\n",
    "        return \"[‚ùå No articles found for this query.]\", urls\n",
    "\n",
    "    print(f\"üìÑ Extracting content from {len(urls)} URLs...\")\n",
    "    sources = []\n",
    "    successful_urls = []\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            raw_text = extract_readable_text(url)\n",
    "            cleaned_text = smart_clean_text(raw_text)\n",
    "            \n",
    "            # üëá Separate each source properly\n",
    "            source_text = f\"Source: {url}\\n---\\n{cleaned_text}\\n\"\n",
    "            sources.append(source_text)\n",
    "\n",
    "            successful_urls.append(url)\n",
    "          \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to extract {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not sources:\n",
    "        return \"[‚ùå No accessible or relevant sources found.]\", urls\n",
    "\n",
    "    answer_style = classify_question_type(query)\n",
    "\n",
    "    # üëá Here, join sources with two newlines between them for clarity\n",
    "    joined_sources = \"\\n\\n\".join(sources)\n",
    "\n",
    "    if answer_style == \"short\":\n",
    "        final_prompt = f\"\"\"You are a precise assistant. Give a short, factual answer based **only** on the provided sources.\n",
    "\n",
    "### Question:\n",
    "{query}\n",
    "\n",
    "### Sources:\n",
    "{joined_sources}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "    else:\n",
    "        final_prompt = f\"\"\"You are an expert crypto analyst and news summarizer.\n",
    "\n",
    "Please answer the following question using only the information from the provided sources.\n",
    "Be clear, concise, and neutral in tone.\n",
    "\n",
    "### Question:\n",
    "{query}\n",
    "\n",
    "### Sources:\n",
    "{joined_sources}\n",
    "\n",
    "### Summary:\n",
    "\"\"\"\n",
    "\n",
    "    print(\"üß† Asking llama3 locally...\")\n",
    "    answer = ask_llama3_locally(final_prompt)\n",
    "    return answer, successful_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d07e0fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f21277d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3852cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4e848f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching with Serper for: What is the price of bitcoin today?\n",
      "üîç Evaluating source: https://coinmarketcap.com/currencies/bitcoin/\n",
      "ü§ñ AI verdict for https://coinmarketcap.com/currencies/bitcoin/: **yes**\n",
      "üîç Evaluating source: https://www.coindesk.com/price/bitcoin\n",
      "ü§ñ AI verdict for https://www.coindesk.com/price/bitcoin: yes\n",
      "üîç Evaluating source: https://bitflyer.com/en-us/bitcoin-chart\n",
      "ü§ñ AI verdict for https://bitflyer.com/en-us/bitcoin-chart: yes\n",
      "üîç Evaluating source: https://www.coinbase.com/price/bitcoin\n",
      "ü§ñ AI verdict for https://www.coinbase.com/price/bitcoin: **no**\n",
      "üîç Evaluating source: https://crypto.com/price/bitcoin\n",
      "ü§ñ AI verdict for https://crypto.com/price/bitcoin: no\n",
      "üîç Evaluating source: https://ca.finance.yahoo.com/quote/BTC-USD/\n",
      "ü§ñ AI verdict for https://ca.finance.yahoo.com/quote/BTC-USD/: yes\n",
      "üìé URLs after blacklist filter: ['https://coinmarketcap.com/currencies/bitcoin/', 'https://www.coindesk.com/price/bitcoin', 'https://ca.finance.yahoo.com/quote/BTC-USD/']\n",
      "üìÑ Extracting content from 3 URLs...\n",
      "üß† Asking llama3 locally...\n",
      "teteteAccording to the provided sources:\n",
      "\n",
      "* CoinMarketCap (https://coinmarketcap.com/currencies/bitcoin/) reports that the current price of Bitcoin is $94,706.51 USD.\n",
      "* CoinDesk (https://www.coindesk.com/price/bitcoin/) does not provide a specific price figure in their website's price section at the time of my knowledge cutoff. However, I do not have access to real-time data and therefore cannot provide a current price based on this source.\n",
      "\n",
      "Please note that prices may fluctuate rapidly and could change by the time you read this answer. For the most up-to-date information, I recommend checking the sources directly.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the price of bitcoin today?\"\n",
    "final_answer,urls= web_rag_query(query)\n",
    "print(\"tetete\"+final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bfde364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching with Serper for: give me the exact price of bitcoin today ?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'web_search_serper' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mgive me the exact price of bitcoin today ?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m final_answer, urls = \u001b[43mweb_rag_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m display_final_answer(final_answer, question, urls)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mweb_rag_query\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mweb_rag_query\u001b[39m(query):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müîç Searching with Serper for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     urls = \u001b[43mweb_search_serper\u001b[49m(query)\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# --- Blacklist bad URLs ---\u001b[39;00m\n\u001b[32m      6\u001b[39m     blacklist = [\n\u001b[32m      7\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://bitflyer.com/en-us/bitcoin-chart\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m     ]\n",
      "\u001b[31mNameError\u001b[39m: name 'web_search_serper' is not defined"
     ]
    }
   ],
   "source": [
    "question = \"give me the exact price of bitcoin today ?\"\n",
    "\n",
    "final_answer, urls = web_rag_query(question)\n",
    "display_final_answer(final_answer, question, urls)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
