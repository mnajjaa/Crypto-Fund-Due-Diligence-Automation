{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c7800b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from newspaper import Article\n",
    "\n",
    "# -------- CONFIG --------\n",
    "SERPER_API_KEY = \"b6377b6d2d07fe6a6f334f03986986351846d4bb\"  # üîÅ Replace with your actual key\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api/generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d694248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6e1158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.51.0)\n",
      "Requirement already satisfied: pyee<13,>=12 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from playwright) (12.1.1)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from playwright) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyee<13,>=12->playwright) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install playwright\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19886214",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2310654265.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mplaywright install\u001b[39m\n               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "playwright install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479046eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (2024.12.14)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (25.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2dd032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_article_text(url):\n",
    "    try:\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return f\"Source: {url}\\n\\n{article.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"Source: {url}\\n\\n[Failed to extract: {e}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "636ee79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llama3_locally(prompt):\n",
    "    try:\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Accept\": \"application/json\"\n",
    "        }\n",
    "        response = requests.post(\n",
    "            OLLAMA_API_URL,\n",
    "            json={\"model\": \"llama3\", \"prompt\": prompt, \"stream\": False},\n",
    "            headers=headers,\n",
    "            verify=False  # Disable SSL verification for ngrok URL\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "                answer_text = response.json().get(\"response\", \"[No response found]\")\n",
    "                return answer_text  # ‚úÖ Return only the clean text\n",
    "        else:\n",
    "            return f\"[HTTP {response.status_code}] Error from Llama3\"\n",
    "    except Exception as e:\n",
    "        return f\"[‚ùå Error calling Llama3: {e}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74bb5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934f88fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66cd05af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    texts = soup.find_all(text=True)\n",
    "\n",
    "    blacklist = [\n",
    "        '[document]', 'noscript', 'header', 'html', 'meta', 'head', 'input', 'script', 'style'\n",
    "    ]\n",
    "\n",
    "    output = ''\n",
    "    for t in texts:\n",
    "        if t.parent.name not in blacklist:\n",
    "            content = t.strip()\n",
    "            if content:\n",
    "                output += f'{content} '\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32a0714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_url_trusted_by_ai(url):\n",
    "    prompt = f\"\"\"You are a trustworthiness evaluator.\n",
    "\n",
    "Decide whether the following website is generally considered a **professional and trustworthy source of information** for serious research, financial news, or due diligence.\n",
    "\n",
    "Do **not** include websites that:\n",
    "- Are editable by the public (like Wikipedia, Reddit, Quora)\n",
    "- Are known for unverified or user-generated content\n",
    "\n",
    "Only reply with `yes` or `no`.\n",
    "\n",
    "### Website:\n",
    "{url}\n",
    "\n",
    "### Is this source trusted?\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Accept\": \"application/json\"\n",
    "        }\n",
    "        response = requests.post(\n",
    "            OLLAMA_API_URL,\n",
    "            json={\"model\": \"llama3\", \"prompt\": prompt, \"stream\": False},\n",
    "            headers=headers,\n",
    "            verify=False\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            verdict = response.json().get(\"response\", \"\").strip().lower()\n",
    "            print(f\"ü§ñ AI verdict for {url}: {verdict}\")\n",
    "            return \"yes\" in verdict\n",
    "        else:\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"[AI trust check failed for {url}]: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c40b2a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search_serper(query, max_results=4):\n",
    "    headers = {\n",
    "        \"X-API-KEY\": SERPER_API_KEY\n",
    "    }\n",
    "    json_data = {\n",
    "        \"q\": query\n",
    "    }\n",
    "    response = requests.post(\"https://google.serper.dev/search\", headers=headers, json=json_data)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå Serper API error: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    data = response.json()\n",
    "    results = data.get(\"organic\", [])\n",
    "    urls = []\n",
    "    for item in results:\n",
    "        url = item[\"link\"]\n",
    "        print(f\"üîç Evaluating source: {url}\")\n",
    "        if is_url_trusted_by_ai(url):\n",
    "            urls.append(url)\n",
    "        if len(urls) >= max_results:\n",
    "            break\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ba2d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_question_type(query):\n",
    "    classification_prompt = f\"\"\"You are a classification assistant.\n",
    "\n",
    "Decide if the following question requires a short, direct answer (like a number, name, or fact), or a detailed, multi-paragraph summary.\n",
    "\n",
    "Respond with only one word: `short` or `detailed`.\n",
    "\n",
    "### Question:\n",
    "{query}\n",
    "\n",
    "### Answer Style:\"\"\"\n",
    "\n",
    "    # Send to LLaMA3\n",
    "    try:\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Accept\": \"application/json\"\n",
    "        }\n",
    "        response = requests.post(\n",
    "            OLLAMA_API_URL,\n",
    "            json={\"model\": \"llama3\", \"prompt\": classification_prompt, \"stream\": False},\n",
    "            headers=headers,\n",
    "            verify=False\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            style = response.json().get(\"response\", \"\").strip().lower()\n",
    "            return \"short\" if \"short\" in style else \"detailed\"\n",
    "        else:\n",
    "            return \"detailed\"\n",
    "    except:\n",
    "        return \"detailed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cf340ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_final_answer(answer, query, urls):\n",
    "    print(\"tetete\"+answer)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"üß† Answer to: {query}\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nüîó Sources Used:\")\n",
    "    for url in urls:\n",
    "        print(f\" - {url}\")\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7928b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "async def extract_readable_text1(url):\n",
    "    try:\n",
    "        # Try normal requests first\n",
    "        response = requests.get(url, timeout=15)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        text = soup.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "        if len(text) < 500:\n",
    "            print(f\"üîÑ Switching to Async Playwright for {url}...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            return await extract_with_playwright_async(url)\n",
    "\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error using requests on {url}: {e}\")\n",
    "        print(f\"üîÑ Switching to Async Playwright for {url}...\")\n",
    "        return await extract_with_playwright_async(url)\n",
    "\n",
    "\n",
    "async def extract_with_playwright_async(url):\n",
    "    try:\n",
    "        async with async_playwright() as p:\n",
    "            browser = await p.chromium.launch(headless=True)\n",
    "            page = await browser.new_page()\n",
    "            await page.goto(url, timeout=60000)\n",
    "            await page.wait_for_load_state('networkidle')\n",
    "\n",
    "            html = await page.content()\n",
    "            await browser.close()\n",
    "\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            text = soup.get_text(separator=\"\\n\").strip()\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Playwright async extraction failed for {url}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965cb9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def extract_readable_text2(url):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.execute_cdp_cmd(\n",
    "        \"Page.addScriptToEvaluateOnNewDocument\",\n",
    "        {\n",
    "            \"source\": \"\"\"\n",
    "                Object.defineProperty(navigator, 'webdriver', {\n",
    "                  get: () => undefined\n",
    "                })\n",
    "            \"\"\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(15)  # Enough for heavy JS\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # üöÄ Only focus on main content tags\n",
    "    whitelist = ['p', 'h1', 'h2', 'h3', 'h4', 'li', 'span', 'a', 'td', 'th', 'strong', 'b']\n",
    "\n",
    "    output = ''\n",
    "    for tag in soup.find_all(whitelist):\n",
    "        text = tag.get_text(strip=True)\n",
    "        if text and len(text) > 5:  # Ignore too short junk\n",
    "            output += f\"{text}\\n\"\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b5c1553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethereum News Today & ETH Predictions | Cointelegraph Ecosystem English News Indices In Depth Learn Newsletters Crypto Bonus Research Podcasts About BTC $94,025 0.46% ETH $1,783 0.21% XRP $2.18 0.66% BNB $601.75 0.55% SOL $148.08 4.25% DOGE $0.181 0.87% ADA $0.7056 1.92% STETH $1,780.71 0.04% TRX $0.251 3.13% AVAX $21.88 3.04% SUI $3.41 8.20% TON $3.21 0.42% BTC $94,013 0.48% ETH $1,782 0.25% XRP $2.18 0.66% BNB $601.68 0.56% SOL $148.10 4.07% DOGE $0.1811 0.69% ADA $0.7058 1.72% STETH $1,780.71 0.04% TRX $0.251 3.13% AVAX $21.90 2.95% SUI $3.42 8.14% TON $3.21 0.36% BTC $94,025 0.46% ETH $1,783 0.20% XRP $2.18 0.62% BNB $601.68 0.56% SOL $148.10 4.07% DOGE $0.1811 0.69% ADA $0.7058 1.72% STETH $1,780.71 0.04% TRX $0.251 3.13% AVAX $21.90 2.95% SUI $3.42 8.14% TON $3.21 0.36% USD Ad Ethereum (ETH) Crypto News Today Ad Win prizes Crypto assets are a high-risk investment. You should consider whether you understand the possibility of losing money due to leverage. None of the material shou\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "\n",
    "def extract_readable_text(url):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--disable-infobars\")\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    # Make webdriver harder to detect\n",
    "    driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n",
    "        \"source\": \"\"\"\n",
    "            Object.defineProperty(navigator, 'webdriver', {get: () => undefined});\n",
    "        \"\"\"\n",
    "    })\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Let the page start rendering\n",
    "\n",
    "    # Scroll slowly to trigger lazy loading elements\n",
    "    scroll_pause_time = 1.0\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    for _ in range(3):  # Scroll down 3 times\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(scroll_pause_time)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    time.sleep(2)  # Wait a little extra to be sure content is loaded\n",
    "\n",
    "    html = driver.page_source\n",
    "    driver.quit()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    texts = soup.find_all(string=True)\n",
    "\n",
    "    output = ''\n",
    "    blacklist = ['[document]', 'noscript', 'header', 'html', 'meta', 'head', 'input', 'script', 'style', 'iframe', 'svg']\n",
    "\n",
    "    for t in texts:\n",
    "        if t.parent.name not in blacklist and t.strip():\n",
    "            output += '{} '.format(t.strip())\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://cointelegraph.com/tags/ethereum\"\n",
    "    text = extract_readable_text(url)\n",
    "    print(text[:1000])  # Show first 1000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3579cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "def extract_readable_text1(url):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    driver.execute_cdp_cmd(\n",
    "        \"Page.addScriptToEvaluateOnNewDocument\",\n",
    "        {\n",
    "            \"source\": \"\"\"\n",
    "                Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\n",
    "            \"\"\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    # Let JS heavy pages load\n",
    "    time.sleep(8)  # 8s is a good compromise\n",
    "\n",
    "    try:\n",
    "        # Get all visible text\n",
    "        visible_texts = driver.execute_script(\"\"\"\n",
    "            let textNodes = [];\n",
    "            function getVisibleTextNodes(node) {\n",
    "                if (node.nodeType === Node.TEXT_NODE && node.nodeValue.trim() !== '') {\n",
    "                    const style = window.getComputedStyle(node.parentElement);\n",
    "                    if (style && style.visibility !== 'hidden' && style.display !== 'none') {\n",
    "                        textNodes.push(node.nodeValue.trim());\n",
    "                    }\n",
    "                }\n",
    "                for (let child of node.childNodes) {\n",
    "                    getVisibleTextNodes(child);\n",
    "                }\n",
    "            }\n",
    "            getVisibleTextNodes(document.body);\n",
    "            return textNodes;\n",
    "        \"\"\")\n",
    "\n",
    "        full_text = \"\\n\".join(visible_texts)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error extracting text: {e}\")\n",
    "        full_text = \"\"\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14247235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def smart_clean_text(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    important_lines = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        # Filter rules:\n",
    "        if \"cookie\" in line.lower():\n",
    "            continue\n",
    "        if \"privacy\" in line.lower():\n",
    "            continue\n",
    "        if \"consent\" in line.lower():\n",
    "            continue\n",
    "        if \"sign up\" in line.lower() or \"sign in\" in line.lower():\n",
    "            continue\n",
    "        if \"accept\" in line.lower() and \"cookies\" in line.lower():\n",
    "            continue\n",
    "        if len(line) < 5:\n",
    "            continue\n",
    "        important_lines.append(line)\n",
    "\n",
    "    cleaned_text = \"\\n\".join(important_lines)\n",
    "    return cleaned_text\n",
    "\n",
    "# Usage\n",
    "\n",
    "# Then build prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "056ff04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_rag_query(query):\n",
    "    print(f\"üîç Searching with Serper for: {query}\")\n",
    "    urls = web_search_serper(query)\n",
    "\n",
    "    # --- Blacklist bad URLs ---\n",
    "    blacklist = [\n",
    "        \"https://bitflyer.com/en-us/bitcoin-chart\"\n",
    "    ]\n",
    "\n",
    "    # Remove blacklisted URLs\n",
    "    urls = [url for url in urls if url not in blacklist]\n",
    "\n",
    "    print(f\"üìé URLs after blacklist filter: {urls}\")\n",
    "    if not urls:\n",
    "        return \"[‚ùå No articles found for this query.]\", urls\n",
    "\n",
    "    print(f\"üìÑ Extracting and summarizing content from {len(urls)} URLs...\")\n",
    "    summaries = []\n",
    "    successful_urls = []\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            raw_text = extract_readable_text(url)\n",
    "\n",
    "            # üß† Now create a query-focused summarization prompt\n",
    "            summarize_prompt = f\"\"\"\n",
    "User question: {query}\n",
    "\n",
    "Summarize only the parts of this page that are relevant to answering the user's question.\n",
    "\n",
    "Page Content:\n",
    "{raw_text}\n",
    "\"\"\"\n",
    "            summary = ask_llama3_locally(summarize_prompt)\n",
    "\n",
    "            source_text = f\"Source: {url}\\n---\\n{summary.strip()}\"\n",
    "            summaries.append(source_text)\n",
    "            print(f\"‚úÖ Successfully summarized content from {url}\")\n",
    "            successful_urls.append(url)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to extract or summarize {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not summaries:\n",
    "        return \"[‚ùå No accessible or relevant sources found.]\", urls\n",
    "\n",
    "    joined_summaries = \"\\n\\n\".join(summaries)\n",
    "\n",
    "    answer_style = classify_question_type(query)\n",
    "\n",
    "    if answer_style == \"short\":\n",
    "        final_prompt = f\"\"\"You are a precise assistant. Give a short, factual answer based **only** on the provided sources.\n",
    "\n",
    "### Question:\n",
    "{query}\n",
    "\n",
    "### Sources:\n",
    "{joined_summaries}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "    else:\n",
    "        final_prompt = f\"\"\"You are an expert crypto analyst and news summarizer.\n",
    "\n",
    "Please answer the following question using only the information from the provided sources.\n",
    "Be clear, concise, and neutral in tone.\n",
    "\n",
    "### Question:\n",
    "{query}\n",
    "\n",
    "### Sources:\n",
    "{joined_summaries}\n",
    "\n",
    "### Summary:\n",
    "\"\"\"\n",
    "\n",
    "    print(\"üß† Asking llama3 locally...\")\n",
    "    answer = ask_llama3_locally(final_prompt)\n",
    "    return answer, successful_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a229b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_rag_query1(query):\n",
    "    \n",
    "    print(f\"üîç Searching with Serper for: {query}\")\n",
    "    urls = web_search_serper(query)\n",
    "\n",
    "    # --- Blacklist bad URLs ---\n",
    "    blacklist = [\n",
    "        \"https://bitflyer.com/en-us/bitcoin-chart\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Remove blacklisted URLs\n",
    "    urls = [url for url in urls if url not in blacklist]\n",
    "\n",
    "    print(f\"üìé URLs after blacklist filter: {urls}\")\n",
    "    if not urls:\n",
    "        return \"[‚ùå No articles found for this query.]\", urls\n",
    "\n",
    "    print(f\"üìÑ Extracting content from {len(urls)} URLs...\")\n",
    "    sources = []\n",
    "    successful_urls = []\n",
    "\n",
    "    for url in urls:\n",
    "        try:\n",
    "            raw_text =  extract_readable_text(url)\n",
    "            \n",
    "            # üëá Separate each source properly\n",
    "            source_text = f\"Source: {url}\\n---\\n{raw_text}\\n\" \n",
    "            sources.append(source_text)\n",
    "            print(f\"‚úÖ Extracted from {sources}\")\n",
    "            successful_urls.append(url)\n",
    "          \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to extract {url}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not sources:\n",
    "        return \"[‚ùå No accessible or relevant sources found.]\", urls\n",
    "\n",
    "    answer_style = classify_question_type(query)\n",
    "\n",
    "    # üëá Here, join sources with two newlines between them for clarity\n",
    "    joined_sources = \"\\n\\n\".join(sources)\n",
    "\n",
    "    if answer_style == \"short\":\n",
    "        final_prompt = f\"\"\"You are a precise assistant. Give a short, factual answer based **only** on the provided sources.\n",
    "\n",
    "### Question:\n",
    "{query}\n",
    "\n",
    "### Sources:\n",
    "{joined_sources}\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "    else:\n",
    "        final_prompt = f\"\"\"You are an expert crypto analyst and news summarizer.\n",
    "\n",
    "Please answer the following question using only the information from the provided sources.\n",
    "Be clear, concise, and neutral in tone.\n",
    "\n",
    "### Question:\n",
    "{query}\n",
    "\n",
    "### Sources:\n",
    "{joined_sources}\n",
    "\n",
    "### Summary:\n",
    "\"\"\"\n",
    "\n",
    "    print(\"üß† Asking llama3 locally...\")\n",
    "    answer = ask_llama3_locally(final_prompt)\n",
    "    return answer, successful_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07e0fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in c:\\users\\msi\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f21277d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28dba916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Searching with Serper for: what is the price of bitcoin today\n",
      "üîç Evaluating source: https://coinmarketcap.com/currencies/bitcoin/\n",
      "ü§ñ AI verdict for https://coinmarketcap.com/currencies/bitcoin/: **no**\n",
      "üîç Evaluating source: https://www.coindesk.com/price/bitcoin\n",
      "ü§ñ AI verdict for https://www.coindesk.com/price/bitcoin: **yes**\n",
      "üîç Evaluating source: https://bitflyer.com/en-us/bitcoin-chart\n",
      "ü§ñ AI verdict for https://bitflyer.com/en-us/bitcoin-chart: yes\n",
      "üîç Evaluating source: https://www.coinbase.com/price/bitcoin\n",
      "ü§ñ AI verdict for https://www.coinbase.com/price/bitcoin: no\n",
      "üîç Evaluating source: https://ca.finance.yahoo.com/quote/BTC-USD/\n",
      "ü§ñ AI verdict for https://ca.finance.yahoo.com/quote/BTC-USD/: yes\n",
      "üîç Evaluating source: https://www.binance.com/en/price/bitcoin\n",
      "ü§ñ AI verdict for https://www.binance.com/en/price/bitcoin: no\n",
      "üîç Evaluating source: https://robinhood.com/us/en/crypto/BTC/\n",
      "ü§ñ AI verdict for https://robinhood.com/us/en/crypto/BTC/: **no**\n",
      "üîç Evaluating source: https://www.tradingview.com/symbols/BTCUSD/\n",
      "ü§ñ AI verdict for https://www.tradingview.com/symbols/BTCUSD/: **no**\n",
      "üîç Evaluating source: https://www.coingecko.com/en/coins/bitcoin\n",
      "ü§ñ AI verdict for https://www.coingecko.com/en/coins/bitcoin: yes\n",
      "üìé URLs after blacklist filter: ['https://www.coindesk.com/price/bitcoin', 'https://ca.finance.yahoo.com/quote/BTC-USD/', 'https://www.coingecko.com/en/coins/bitcoin']\n",
      "üìÑ Extracting and summarizing content from 3 URLs...\n",
      "‚úÖ Successfully summarized content from https://www.coindesk.com/price/bitcoin\n",
      "‚úÖ Successfully summarized content from https://ca.finance.yahoo.com/quote/BTC-USD/\n",
      "‚úÖ Successfully summarized content from https://www.coingecko.com/en/coins/bitcoin\n",
      "üß† Asking llama3 locally...\n",
      "teteteAccording to the provided sources, the current price of Bitcoin (BTC-USD) is **94,243.32**. Please note that this information may be subject to change as markets fluctuate.\n"
     ]
    }
   ],
   "source": [
    "query= \"what is the price of bitcoin today\"\n",
    "final_answer, urls =  web_rag_query(query)\n",
    "print(\"tetete\"+final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
